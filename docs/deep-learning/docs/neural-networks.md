# 神经网络和反向传播算法

## 神经元

神经元和感知器本质是一样的，，只不过神经网络的激活函数往往选择 sigmoid 函数或者 tanh 函数，如下图所示：

![图 8](/deep-learning/%E7%A5%9E%E7%BB%8F%E5%85%83.png)

计算一个神经元的输出的方法和计算一个感知器的输出是一样的，假设神经元输入是向量 x，权重向量是 w，w0 是 偏置项，激活函数是 sigmoid 函数，则输出 y 的表达式为：

![图 9](/deep-learning/%E7%A5%9E%E7%BB%8F%E5%85%83%E8%BE%93%E5%87%BAy.png)

sigmoid 函数的定义如下：

![图 10](/deep-learning/sigmoid%E5%87%BD%E6%95%B0%E5%AE%9A%E4%B9%89.png)

带入前面的式子可以得到：

![图 11](/deep-learning/%E5%B8%A6%E5%85%A5sigmoid.png)

sigmoid 函数是一个非线性函数，值域是 (0, 1)，函数图像如下：

![图 12](/deep-learning/sigmoid%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F.png)

sigmoid 函数的导数是：

![图 13](/deep-learning/sigmoid%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0.png)

## 神经网络

![图 2](/deep-learning/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%98%AF%E5%95%A5.png)

神经网络就是按照一定规则连接起来的多个神经。上图展示了一个全连接神经网络，通过上面的图，我们可以发现它的规则：

- 神经元按照层来布局，最左边的层叫做输入层，负责接收输入数据，最右边的层叫做输出层，我们可以从这层获取神经网络的输出数据，输入层和输出层之间的层叫做隐藏层，因为它们对于外部不可见。
- 同一层的神经元之间没有连接。
- 第 N 层的每个神经元和第 N-1 层的所有神经元相连，第 N-1 层神经元的输出就是第 N 层神经元的输入。
- 每个连接都有一个权值。

上面这些规则定义了全连接神经网络的结构，实际上还存在很多其他结构的神经网络，比如卷积神经网络 CNN、循环神经网络 RNN，它们都有不同的连接规则。

## 计算神经网络的输出

神经网络实际上就是一个输入向量 x 到输出向量 y 的函数，下面举一个例子，先给神经网络神经网络的每个单元写上编号：

![图 3](/deep-learning/%E8%AE%A1%E7%AE%97%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%BE%93%E5%87%BA.png)

可以看到，隐藏层的节点 4 和输入层的三个节点 1， 2， 3 之间都有连接，连接上的权重分别是 w41， w42， w43，下面计算节点 4 的输出 a4：

为了计算节点 4 的输出值，我们必须先得到节点 4 的全部上有节点也就是 1， 2， 3 的输出值，这三个节点是输入层节点，因此它们的输出就是 x1， x2， x3，这样就可以计算出节点 4 的输出值 a4：

![图 4](/deep-learning/%E8%AE%A1%E7%AE%97a4.png)

w4b 是节点 4 的偏置项，w41， w42， w43 分别是节点 1、2、3 到节点 4 的连接的权重。同样，我们可以计算出其他三个隐藏层节点的输出 a5、a6、a7，这样就确定了隐藏层的四个节点的输出值，由此可以得出输出层节点 8 的输出：

![图 5](/deep-learning/%E8%8A%82%E7%82%B98%E8%BE%93%E5%87%BA.png)

同理也可以计算出输出层节点 9 的输出值。

### 神经网络的矩阵表示

首先列出隐藏层的四个节点的计算：

![图 6](/deep-learning/%E9%9A%90%E8%97%8F%E5%B1%82%E5%9B%9B%E8%8A%82%E7%82%B9%E8%AE%A1%E7%AE%97.png)

接着定义网络的输入向量 x 和隐藏层每个节点的权重向量 w：

![图 7](/deep-learning/%E8%BE%93%E5%85%A5%E5%90%91%E9%87%8F%E5%92%8C%E6%AF%8F%E4%B8%AA%E8%8A%82%E7%82%B9%E7%9A%84%E6%9D%83%E9%87%8D%E5%90%91%E9%87%8F.png)

带入前面的式子可以得到：

![图 9](/deep-learning/%E5%9B%9B%E4%B8%AA%E8%8A%82%E7%82%B9%E7%9A%84%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA.png)

将这四个式子写到矩阵中可以得到：

![图 10](/deep-learning/%E5%BC%8F2.png)

上面的式子中，f 是激活函数，本例中是 sigmoid 函数，W 是某一层的权重矩阵，x 是某层的输入向量，a 是某层的输出向量。这个式子说明神经网络的每一层的作用实际上就是先将输入向量左乘一个数组进行线性变换，得到一个新向量，然后再对这个向量主元素应用激活函数。

因此，对于一个包含一个输入层，一个输出层和三个隐藏层的神经网络，假设权重矩阵为 W1、W2、W3、W4，每个隐藏层的输出分别是 a1、a2、a3，神经网络的输入为 x，输出为 y，则每一层的输出想来那个的计算可以表示为：

![图 11](/deep-learning/%E6%AF%8F%E4%B8%80%E5%B1%82%E7%9A%84%E8%BE%93%E5%87%BA%E5%90%91%E9%87%8F.png)

## 神经网络的训练

现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个模型，那么这些权值就是模型的参数，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为超参数。

### 反向传播算法

我们假设每个训练样本是(x, t)，其中向量 x 是训练样本的特征，t 是目标值：

![图 12](/deep-learning/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.png)

根据前面的内容，计算出每个节点的误差：

对于输出层节点：

![图 13](/deep-learning/%E8%BE%93%E5%87%BA%E5%B1%82%E8%8A%82%E7%82%B9%E8%AF%AF%E5%B7%AE.png)

对于隐藏层节点：

![图 14](/deep-learning/%E9%9A%90%E8%97%8F%E5%B1%82%E8%8A%82%E7%82%B9%E8%AF%AF%E5%B7%AE.png)

其中 ai 是节点 i 的输出值，wki 是节点 i 到它下一层的节点 k 的连接的权重，例如，对于隐藏层节点 4 来说：

![图 15](/deep-learning/%E9%9A%90%E8%97%8F%E5%B1%82%E8%8A%82%E7%82%B94%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97.png)

然后更新每个连接上的权值：

![图 16](/deep-learning/%E6%9B%B4%E6%96%B0%E6%AF%8F%E4%B8%AA%E8%BF%9E%E6%8E%A5%E7%9A%84%E6%9D%83%E5%80%BC.png)

例如 w84 的更新方法如下：

![图 17](/deep-learning/w84%E6%9D%83%E9%87%8D%E6%9B%B4%E6%96%B0.png)
