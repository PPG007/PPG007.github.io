# 线性单元和梯度下降

当数据集不是线性可分的时候，感知器规则可能无法收敛，这意味着我们永远也无法完成一个感知器的训练，为了解决这个问题，我们使用一个可导的线性函数来替代感知器的阶跃函数，这种感知器就叫做线性单元。

我们可以设置一个简单的线性单元的激活函数：

![图 1](/deep-learning/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0.png)

这样的线性单元如下所示：

![图 2](/deep-learning/%E7%AE%80%E5%8D%95%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83.png)

这样在替换了激活函数 f 之后，线性单元将返回一个实数值而不是 0，1 分类，因此线性单元用来解决回归问题而不是分类问题。

## 线性单元的模型

模型，实际上就是根据输入预测输出的算法，例如，x 可以是一个人的工作年限，y 可以是他的月薪，我们可以用某种算法来根据一个人的工作年限来预测他的收入，例如下面这个公式：

![图 3](/deep-learning/%E7%AE%80%E5%8D%95%E9%A2%84%E6%B5%8B%E6%9C%88%E8%96%AA.png)

函数 h(x) 叫做假设，w、b 是它的参数，假设参数 w = 1000，参数 b = 500，那么可以计算出 y = 5500。

但是这个模型考虑的因素很少，导致预测结果不是很可靠，如果考虑所在行业、公司、职级等等因素可能会使预测结果更加准确，这些因素就称为特征，对于一个工作了 5 年，在 IT 行业，百度工作，职级 T6 这样的人，我们可以用下面这样的一个特征向量来表示：x = (5, IT, baidu, T6)。

由于输入变成了一个具备四个特征的向量，所以还需要引入 w1, w2, w3, w4，，这样模型就变成了下面这样：

![图 1](/deep-learning/%E5%9B%9B%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B.png)

其中 x1 对应工作年限，x2 对应行业， x3 对应公司，x4 对应职级。为了书写方便可以令 w0 等于 b，同时令 w0 对应的特征是 x0，由于 x0 并不存在，可以令 x0 = 1，这样上面的公式可以写作：

![图 1](/deep-learning/四特征向量模型+b.png)

也可以提取出 w，写作：

![图 3](/deep-learning/%E6%96%AF%E7%89%B9%E6%AD%A3%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%E5%90%91%E9%87%8F%E5%BD%A2%E5%BC%8F.png)

形如这样的模型就叫做线性模型，因为输出 y 就是输入特征 x 的 线性组合。

## 监督学习和无监督学习

监督学习：为了训练一个模型，需要提供一堆训练样本，每个训练样本既包括输入特征 x，也包括对应的输出 y，y 也叫标记 label。当模型看过足够多的样本之后，就可以总结出一些规律，然后就可以进行预测了。

无监督学习：训练样本中只有输入没有输出，模型可以总结出特征 x 的一些规律但是无法知道其对应的答案 y。

## 线性单元的目标函数

在监督学习下，对于一个样本，我们知道输入以及标记，同时我们还可以根据模型函数计算出输出的预测值，预测值与输入值越接近越好。

表示两个值接近程度的数学方法有很多，例如：

![图 4](/deep-learning/%E6%8E%A5%E8%BF%91%E7%A8%8B%E5%BA%A6.png)

e 称为单个样本的误差，训练数据中会有很多样本，我们将所有样本的误差求和来表示模型的误差 E。对于一个确定的训练集来说，输入、输出都是确定的，w 是未知的，因此误差 E 其实是关于 w 的函数。因此，模型的训练就是求取到合适的 w，使得 E 取最小值，E(w) 就是我们优化的目标，称为目标函数。

## 梯度下降优化算法

![图 5](/deep-learning/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.png)

计算机可以通过计算能力不断的实验 能够使得 y 值更小的 x 的取值。我们首先随便选择一个点，例如上面的 x0，然后对 x0 进行迭代，经过数次迭代之后达到函数最小值点。

如何使得每次修改 x 的值都能向函数最小值呢？就是每次都是向函数的梯度的反方向修改 x。梯度是一个向量，指向函数数值上升最快的方向，显然，梯度的反方向就是函数值下降的最快的方向了。下面就是梯度下降算法的函数：

![图 6](/deep-learning/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%87%BD%E6%95%B0.png)

经过一些列推导（此处略），得到上面线性单元的参数修改规则：

![图 7](/deep-learning/%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%E5%8F%82%E6%95%B0%E4%BF%AE%E6%94%B9%E8%A7%84%E5%88%99.png)

## 随机梯度下降算法 SGD

如果我们根据上面的线性单元参数修改规则来训练模型，那么我们每次更新的迭代，要遍历训练数据中所有的样本进行计算，我们称这种算法叫做批梯度下降(Batch Gradient Descent， BGD)。如果我们的样本非常大，比如数百万到数亿，那么计算量异常巨大。因此，实用的算法是SGD算法。在SGD算法中，每次更新 w 的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对 w 更新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新并不一定按照 E 减少的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少 E 的方向前进的，因此最后也能收敛到最小值附近。

## Python 实现线性单元

和此前的感知器相比，线性单元唯一不同的就是激活函数，因此可以复用之前的代码：

```python
from main import Perceptron


class LinearUnit(Perceptron):
    def __init__(self, input_num, activator):
        super().__init__(input_num, activator)


def get_training_dataset():
    input_vecs = [[5, 6], [3, 8], [8, 9], [1.4, 11.4], [10.1, 3.14]] # 这里可以是多维的
    labels = [5500, 2300, 7600, 1800, 11400]
    return input_vecs, labels


def train_linear():
    lu = LinearUnit(2, lambda x: x)
    input_vecs, labels = get_training_dataset()
    lu.train(input_vecs, labels, 10, 0.01)
    return lu


if __name__ == '__main__':
    linear_unit = train_linear()
    # 打印训练获得的权重
    print(linear_unit)
    # 测试
    print('Work 3.4 years, monthly salary = %.2f' % linear_unit.predict([3.4]))
    print('Work 15 years, monthly salary = %.2f' % linear_unit.predict([15]))
    print('Work 1.5 years, monthly salary = %.2f' % linear_unit.predict([1.5]))
    print('Work 6.3 years, monthly salary = %.2f' % linear_unit.predict([6.3]))
```
